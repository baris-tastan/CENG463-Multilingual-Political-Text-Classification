{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10385204,"sourceType":"datasetVersion","datasetId":6433537}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf\")\nlogin(token=secret_value_0)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jLA-OA22zBE","outputId":"4e6aa8ee-082b-4550-ebe5-12d629ce3e14","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:23:23.065461Z","iopub.execute_input":"2025-01-25T12:23:23.065803Z","iopub.status.idle":"2025-01-25T12:23:23.457909Z","shell.execute_reply.started":"2025-01-25T12:23:23.065773Z","shell.execute_reply":"2025-01-25T12:23:23.457180Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification\n\n# Load dataset\ndata = pd.read_csv(\"/kaggle/input/orientationtr/orientation-tr-train.tsv\", sep=\"\\t\")\n\n# Preprocessing: Handle missing values\ndata = data.dropna(subset=[ 'text_en'])\n\n# Stratified train-test split\ntrain_data, val_data = train_test_split(\n    data, test_size=0.1, stratify=data[\"label\"], random_state=42\n)\n\n# Save the train and validation datasets\ntrain_data.to_csv(\"train_data.tsv\", sep=\"\\t\", index=False)\nval_data.to_csv(\"val_data.tsv\", sep=\"\\t\", index=False)\n\n# Check class distribution\nlabel_counts = data[\"label\"].value_counts()\n\n# Print class distribution\nprint(\"Class Distribution:\")\nprint(label_counts)\n\n# Calculate imbalance ratio\ntotal_samples = label_counts.sum()\nimbalance_ratios = label_counts / total_samples\nprint(\"\\nClass Imbalance Ratios:\")\nprint(imbalance_ratios)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yLERjlEJGjk","outputId":"6d98ff7c-9b20-480b-b94b-55c35da2bea8","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:23:16.127075Z","iopub.execute_input":"2025-01-25T12:23:16.127426Z","iopub.status.idle":"2025-01-25T12:23:20.295561Z","shell.execute_reply.started":"2025-01-25T12:23:16.127396Z","shell.execute_reply":"2025-01-25T12:23:20.294541Z"}},"outputs":[{"name":"stdout","text":"Class Distribution:\nlabel\n1    9390\n0    6748\nName: count, dtype: int64\n\nClass Imbalance Ratios:\nlabel\n1    0.581856\n0    0.418144\nName: count, dtype: float64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification\n\n\n# Load the saved datasets\ntrain_data = pd.read_csv(\"train_data.tsv\", sep=\"\\t\")\nval_data = pd.read_csv(\"val_data.tsv\", sep=\"\\t\")\n\nprint(\"Datasets loaded successfully.\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\ndef tokenize_data(texts, labels):\n    return tokenizer(\n        list(texts), padding=True, truncation=True, return_tensors=\"pt\"\n    ), labels\n\ntrain_tokens, train_labels = tokenize_data(train_data[\"text_en\"], train_data[\"label\"])\nval_tokens, val_labels = tokenize_data(val_data[\"text_en\"], val_data[\"label\"])\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2, torch_dtype=\"auto\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:23:29.978094Z","iopub.execute_input":"2025-01-25T12:23:29.978448Z","iopub.status.idle":"2025-01-25T12:23:50.129676Z","shell.execute_reply.started":"2025-01-25T12:23:29.978421Z","shell.execute_reply":"2025-01-25T12:23:50.128976Z"}},"outputs":[{"name":"stdout","text":"Datasets loaded successfully.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\nfrom transformers import TrainingArguments, Trainer\n","metadata":{"id":"OmcHMf4GdPtW","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:24:06.174718Z","iopub.execute_input":"2025-01-25T12:24:06.175056Z","iopub.status.idle":"2025-01-25T12:24:07.330431Z","shell.execute_reply.started":"2025-01-25T12:24:06.175025Z","shell.execute_reply":"2025-01-25T12:24:07.329758Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nfrom datasets import Dataset\n\n# Combine the tokenized data into datasets\ntrain_dataset = Dataset.from_dict({\n    \"input_ids\": train_tokens[\"input_ids\"],\n    \"attention_mask\": train_tokens[\"attention_mask\"],\n    \"labels\": train_labels,\n})\n\neval_dataset = Dataset.from_dict({\n    \"input_ids\": val_tokens[\"input_ids\"],\n    \"attention_mask\": val_tokens[\"attention_mask\"],\n    \"labels\": val_labels,\n})\n\ntraining_args = TrainingArguments(\n    output_dir=\"./orientation-tr\",\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    logging_dir='./logs',\n    logging_steps=50,\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\n\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"id":"3cJONibOdcr2","outputId":"e0912f81-026b-47d7-a2b0-903baba058cd","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T12:24:43.153687Z","iopub.execute_input":"2025-01-25T12:24:43.154120Z","iopub.status.idle":"2025-01-25T13:20:50.038461Z","shell.execute_reply.started":"2025-01-25T12:24:43.154081Z","shell.execute_reply":"2025-01-25T13:20:50.037693Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='908' max='908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [908/908 56:02, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.413400</td>\n      <td>0.361027</td>\n      <td>0.837670</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.320400</td>\n      <td>0.305267</td>\n      <td>0.870508</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.220700</td>\n      <td>0.316690</td>\n      <td>0.867410</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.128800</td>\n      <td>0.360620</td>\n      <td>0.871128</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=908, training_loss=0.29103502670573766, metrics={'train_runtime': 3365.2687, 'train_samples_per_second': 17.263, 'train_steps_per_second': 0.27, 'total_flos': 1.528569987219456e+16, 'train_loss': 0.29103502670573766, 'epoch': 4.0})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf\")\nlogin(token=secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset\nimport pandas as pd\n\n\nmodel_id = \"meta-llama/Llama-3.2-1B\"\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=model_id, device_map=\"auto\")\n\n# Define candidate labels\ncandidate_labels = [\"left wing\", \"right wing\"]\n\n# Predict labels using the classifier\npredictions = []\ni = 0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HADxKgXgX-X7","outputId":"2f519622-d434-42ac-f6fa-15ff16d297b6","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T13:24:22.647248Z","iopub.execute_input":"2025-01-25T13:24:22.647567Z","iopub.status.idle":"2025-01-25T13:25:57.409608Z","shell.execute_reply.started":"2025-01-25T13:24:22.647543Z","shell.execute_reply":"2025-01-25T13:25:57.408856Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff6d0ca885634fd08f4ace5a5f7238a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a5391d442cd4f50b2a8100622d62800"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f676d0be179445ea882091f94d6dd12c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb4992db5ab142ffb1cf80f11ae38fb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf3ceea49984a07b0309bf79c767b9c"}},"metadata":{}},{"name":"stderr","text":"Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"\npredictions = []\nfor text in val_data[\"text\"]:\n    result = classifier(text, candidate_labels=candidate_labels)\n    if 'labels' in result:\n      predictions.append(result['labels'][0])\n\n\n# Map predictions and true labels to a common format\n# Assuming `data['label']` has values like 0 for \"right wing\" and 1 for \"left wing\"\ntrue_labels = val_data[\"label\"].map({1: \"right wing\", 0: \"left wing\"})\n \nfrom sklearn.metrics import accuracy_score\n\n# Map the predicted labels back to 0 and 1\nlabel_mapping = {\"left wing\": 0, \"right wing\": 1}\npredictions_mapped = [label_mapping[label] for label in predictions]\n\n# Calculate accuracy\naccuracy = accuracy_score(val_data[\"label\"], predictions_mapped)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T13:26:00.165782Z","iopub.execute_input":"2025-01-25T13:26:00.166088Z","iopub.status.idle":"2025-01-25T14:02:39.185016Z","shell.execute_reply.started":"2025-01-25T13:26:00.166064Z","shell.execute_reply":"2025-01-25T14:02:39.184071Z"}},"outputs":[{"name":"stderr","text":"Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.4232\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\n\npredictions = []\n\n# Loop through val_data[\"text_en\"] with a progress bar\nfor text in tqdm(val_data[\"text_en\"], desc=\"Processing texts\"):\n    result = classifier(text, candidate_labels=candidate_labels)\n    if 'labels' in result:\n        predictions.append(result['labels'][0])\n\ntrue_labels = val_data[\"label\"].map({1: \"right wing\", 0: \"left wing\"})\n\n# Map the predicted labels back to 0 and 1\nlabel_mapping = {\"left wing\": 0, \"right wing\": 1}\npredictions_mapped = [label_mapping[label] for label in predictions]\n\n# Calculate accuracy\naccuracy = accuracy_score(val_data[\"label\"], predictions_mapped)\nprint(f\"Accuracy: {accuracy:.4f}\")\n","metadata":{"id":"NSNgGYHU-HLm","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:38:04.025145Z","iopub.execute_input":"2025-01-25T16:38:04.025478Z","iopub.status.idle":"2025-01-25T17:08:14.485822Z","shell.execute_reply.started":"2025-01-25T16:38:04.025450Z","shell.execute_reply":"2025-01-25T17:08:14.485060Z"}},"outputs":[{"name":"stderr","text":"Processing texts: 100%|██████████| 1614/1614 [30:10<00:00,  1.12s/it]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.4504\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":18}]}